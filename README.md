# Data modeling with Postgres
This project focuses on building a star schema from a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/) and log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above.

## Installation

Run the two commands in the *run.ipynb* to create the necessary tables and run the python script.
Alternatively you can use pip or conda to download the necessary modules and run it locally (
```bash
pip install -r requirements.txt && python etl.py
```

## Purpose
This database that is being created helps the fictional company Sparkify create a fact and dimensions (star schema). That enables to:
* Supporting de-normalized data
* Enforcing referential integrity
* Reducing the time required to load large batches of data into a database 
* Improving query performance, i.e. queries generally run faster
* Enabling dimensional tables to be easily updated
* Enabling new ‘facts’ to be easily added i.e. regularly or selectively
* Simplicity i.e. navigating through the data is easy (analytical queries are easier)

## Database design
For the database the decision was made to create 5 tables with 1 fact table (songplays ) storing the fake events and 4 dimension tables (artists,songs,users,time)

#### Songplays
This is the entrypoint for all the potential analytical queries. The decision for the table was to use a serial auto incrementing id for the primary key. 
The table references other tables via song_id and artist_id but the decision here was not to create any foreign key constrains as the lack of data here would mean only very few entries would be created.
NON NULL value types were selected for the start_time and user_id, song_id and artist_id were left as text with the ability to use null values
as the information might be missing
As for the primary key its known that serial creates gaps when the transaction is rolled back due some error but it was used simplicity .
For the type of values I went with int and text in most cases of numbers and strings apart for some cases that bigint was used (start_time) and varchars (if the options of level are limited we could use something like varchar(5), if only paid and free are the valid values)

#### Time table
This table contains the timestamp of the event in the log file and the equivalent datetime represenation of that (used UTC) in hours, days, months, years, weekdays. For most of the columns smallints were used as the values are sufficently small and can save storage space

### User table
This table contains basic data for the user that produced the log event. Again a serial was used as a primary key (we could also use some form of UUID as it is the standard practice for use management)

#### Artists table
This table contains the artist information as given by the song data files. The artist_id coming from the song_data was used as the primary key in this case as it was already unique

#### Songs table
This table contais the song information. The song_id was used as key and the duration as a numeric (potentially we could also round the number and use a small int)

#### INSERTS
I used the ON DUPLICATE KEY DO NOTHING to avoid errors due to rerunning the scripts. 


## ETL

The script is divided in two parts, reading the song data and persistng to the database and reading the log events and persisting as well. The filepath is extracted for all underlying files and pandas are used to handle the json files.
First we read the song data and persist to the song and artist tables. Next the log files are proccessed one by one and for each row we issue a query to figure out the song_id and artist_id from joining the songs and artists table. The results are used in conjunction with the log event to persist to the songplay table. Since a subset of data is provided there is only one song-artist tuple that properly exists in our tables and will be referenced in the songplay table. All other records will have no ids for song and artist.

## Future imrpovements 
* Persisting is done row by row but the throughput could be imrpoved by doing batch inserts
* The data could be moved to a distributed storage system like s3 instead of local directories
* When we define Sparkify's most used queries secondary indexes can help speed up the queries